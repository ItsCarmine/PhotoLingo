{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-07T02:42:28.048609900Z",
     "start_time": "2023-12-07T02:42:20.616102300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import time\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalizing the images\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T02:42:28.064802100Z",
     "start_time": "2023-12-07T02:42:28.053740100Z"
    }
   },
   "id": "3fc88e5958c9b74a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load the training and testing datasets\n",
    "train_dataset = datasets.ImageFolder(root='../dataset/training', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='../dataset/testing', transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T02:42:28.570241200Z",
     "start_time": "2023-12-07T02:42:28.062304200Z"
    }
   },
   "id": "840a1696d84ff440"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # Assuming your input images are 128x128, after three pooling layers, the size will be 128/(2^3) = 16\n",
    "        # Thus, the output of the last conv layer would be [batch_size, 64, 16, 16]\n",
    "        self.fc = nn.Linear(64 * 16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 16 * 16)  # Flatten the output for the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T02:42:28.570241200Z",
     "start_time": "2023-12-07T02:42:28.563444100Z"
    }
   },
   "id": "42de330510bfc554"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "num_classes = len(train_dataset.classes)\n",
    "model = BasicCNN(num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T02:42:28.602389900Z",
     "start_time": "2023-12-07T02:42:28.572583100Z"
    }
   },
   "id": "962306ad56a65bbc"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "BasicCNN(\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc): Linear(in_features=16384, out_features=5, bias=True)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model state\n",
    "model.load_state_dict(torch.load('../models/PhotoLingo_Base_v1.pth'))\n",
    "\n",
    "# Put the model in evaluation mode if you are doing inference\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T02:42:28.651861100Z",
     "start_time": "2023-12-07T02:42:28.610125200Z"
    }
   },
   "id": "4d4f5c11750e1f59"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Checking to make sure we are using our GPU instead of CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T02:42:28.784850900Z",
     "start_time": "2023-12-07T02:42:28.648878900Z"
    }
   },
   "id": "540804c62ee6be6c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carmi\\OneDrive\\Documents\\GitHub\\PhotoLingo\\venv\\Lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 94.26944813829788%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {100 * correct / total}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T02:46:20.427548Z",
     "start_time": "2023-12-07T02:42:28.733302200Z"
    }
   },
   "id": "7b6f955a49202d20"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carmi\\OneDrive\\Documents\\GitHub\\PhotoLingo\\venv\\Lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 1285     1    11    10   103]\n",
      " [    0  1114     6     8    52]\n",
      " [    8     7  1422    66   279]\n",
      " [    9     4    63  1731   229]\n",
      " [   88    35   239   161 17133]]\n",
      "Accuracy: 0.9426944813829787%\n",
      "Precision (per class): [0.92446043 0.95951766 0.81677197 0.87601215 0.96274444]\n",
      "Recall (per class): [0.91134752 0.9440678  0.7979798  0.85019646 0.97037834]\n",
      "F1 Score (per class): [0.91785714 0.95173003 0.80726653 0.86291127 0.96654632]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# No gradients needed for evaluation\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())  # Append batch predictions\n",
    "        y_true.extend(labels.cpu().numpy())  # Append true labels\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Calculate precision, recall, F1-score and support for each class\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = np.sum(np.diag(conf_matrix)) / np.sum(conf_matrix)\n",
    "\n",
    "\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "print(f'Accuracy: {accuracy}%')\n",
    "print(f'Precision (per class): {precision}')\n",
    "print(f'Recall (per class): {recall}')\n",
    "print(f'F1 Score (per class): {f1_score}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T02:49:38.084735100Z",
     "start_time": "2023-12-07T02:46:20.439059400Z"
    }
   },
   "id": "bc6d669e864c3d3b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity (per class): [0.99536506 0.99794616 0.98568351 0.98887779 0.89653558]\n",
      "Specificity (avg): 0.9728816207963551\n"
     ]
    }
   ],
   "source": [
    "# Corrected specificity calculation for each class\n",
    "specificity = []\n",
    "num_classes = len(conf_matrix)  # Assuming this is 5 in your case\n",
    "for i in range(num_classes):\n",
    "    true_negative = np.sum(conf_matrix) - np.sum(conf_matrix[i, :]) - np.sum(conf_matrix[:, i]) + conf_matrix[i, i]\n",
    "    false_positive = np.sum(conf_matrix[:, i]) - conf_matrix[i, i]\n",
    "    specificity_class_i = true_negative / (true_negative + false_positive) if (true_negative + false_positive) != 0 else 0\n",
    "    specificity.append(specificity_class_i)\n",
    "\n",
    "# Rest of the code remains the same\n",
    "\n",
    "# Printing results\n",
    "print(f'Specificity (per class): {np.array(specificity)}')\n",
    "print(f'Specificity (avg): {np.mean(np.array(specificity))}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T02:49:38.119960400Z",
     "start_time": "2023-12-07T02:49:38.097757700Z"
    }
   },
   "id": "6db3349bcdaeeaee"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(directory)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.directory, self.images[idx])\n",
    "        image = Image.open(img_path).convert('RGB')  # Load as PIL Image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.images[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T02:49:50.861593200Z",
     "start_time": "2023-12-07T02:49:50.835732200Z"
    }
   },
   "id": "e633010f20ffe6fc"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_dataset = UnlabeledDataset('../dataset/testing_ICDAR', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "predictions = []\n",
    "for inputs, image_names in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    class_names = ['Arabic', 'Hindi', 'Japanese', 'Korean', 'Latin'] \n",
    "    predicted_classes = [class_names[p] for p in preds.cpu()]\n",
    "    \n",
    "    for img_name, prediction in zip(image_names, predicted_classes):\n",
    "        predictions.append(f\"{img_name},{prediction}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T03:15:35.198730300Z",
     "start_time": "2023-12-07T03:02:23.209209200Z"
    }
   },
   "id": "586a740639c30ead"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "with open('predictions.txt', 'w') as f:\n",
    "    for line in predictions:\n",
    "        f.write(line + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T03:15:35.384487500Z",
     "start_time": "2023-12-07T03:15:35.210598500Z"
    }
   },
   "id": "5efb54099f6d89fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2846c0e7c8676ec7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
